{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ebb65a-ba2e-4234-8841-b2777b69472e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import pyreft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a8fd8d5-5e22-4ed0-92bc-fd70f70b41ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea288d3f4d54acba55da8f43ae2b028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63d18b4dbc34ad582c7920472f132c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers, pyreft\n",
    "device = \"cuda\"\n",
    "\n",
    "model_name_or_path = \"google/gemma-2-2b-it\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=2048, \n",
    "    padding_side=\"right\", use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687947c9-cd53-4f7c-a028-f32e2bcc713c",
   "metadata": {},
   "source": [
    "### Specify your job parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d102207-f1ae-4a4f-a2cd-7c80b0ba5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run random baseline to get these files first!\n",
    "n_training_qIDs = \"train_qIDs.json\"\n",
    "n_testing_qIDs = \"test_qIDs.json\"\n",
    "\n",
    "# demographic group and output type\n",
    "demographic_group = \"POLPARTY\"\n",
    "demographic = \"Republican\"\n",
    "output_type = \"sequence\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c7396-7dcb-405c-989a-075a076150e5",
   "metadata": {},
   "source": [
    "#### Getting a random qID and the corresponding training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b741c167-cd80-4cfa-8d58-15537c59c6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/juice2/scr2/wuzhengx/cs329h/Benchmarking-Distributional-Alignment-of-Small-Language-Models/utils.py:138: RuntimeWarning: invalid value encountered in divide\n",
      "  icl_values = np.array(icl_values)/np.sum(icl_values)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>qID</th>\n",
       "      <th>icl_qID</th>\n",
       "      <th>demographic_group</th>\n",
       "      <th>demographic</th>\n",
       "      <th>output_type</th>\n",
       "      <th>wave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nPlease simulate 30 sample...</td>\n",
       "      <td>Answer: D B D D D D C D D B B E D B D B E E D ...</td>\n",
       "      <td>SOCIETY_SSM_W92</td>\n",
       "      <td>GAYMARR2_W32</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nPlease simulate 30 sample...</td>\n",
       "      <td>Answer: C B E C E D E C E B D E D C B C E D D ...</td>\n",
       "      <td>SOCIETY_SSM_W92</td>\n",
       "      <td>FAMSURV6_W50</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nPlease simulate 30 sample...</td>\n",
       "      <td>Answer: C D C A A A D A A A A D E D E D C D D ...</td>\n",
       "      <td>SOCIETY_SSM_W92</td>\n",
       "      <td>SOCIETY_RHIST_W92</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <start_of_turn>user\\nPlease simulate 30 sample...   \n",
       "1  <start_of_turn>user\\nPlease simulate 30 sample...   \n",
       "2  <start_of_turn>user\\nPlease simulate 30 sample...   \n",
       "\n",
       "                                              output              qID  \\\n",
       "0  Answer: D B D D D D C D D B B E D B D B E E D ...  SOCIETY_SSM_W92   \n",
       "1  Answer: C B E C E D E C E B D E D C B C E D D ...  SOCIETY_SSM_W92   \n",
       "2  Answer: C D C A A A D A A A A D E D E D C D D ...  SOCIETY_SSM_W92   \n",
       "\n",
       "             icl_qID demographic_group demographic output_type  \\\n",
       "0       GAYMARR2_W32          POLPARTY  Republican    sequence   \n",
       "1       FAMSURV6_W50          POLPARTY  Republican    sequence   \n",
       "2  SOCIETY_RHIST_W92          POLPARTY  Republican    sequence   \n",
       "\n",
       "                                         wave  \n",
       "0  Pew_American_Trends_Panel_disagreement_100  \n",
       "1  Pew_American_Trends_Panel_disagreement_100  \n",
       "2  Pew_American_Trends_Panel_disagreement_100  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_qIDs = json.load(open(n_training_qIDs))\n",
    "\n",
    "qID_datasets = []\n",
    "for qID in sampled_qIDs:\n",
    "    qID_dataset = get_few_shot_training_examples(\n",
    "        qID,\n",
    "        wave=\"Pew_American_Trends_Panel_disagreement_100\", \n",
    "        demographic_group=demographic_group,\n",
    "        demographic=demographic,\n",
    "        output_type=output_type, \n",
    "        dataset=\"opinionqa\",\n",
    "        n_shots=5,\n",
    "        n_simulations_per_shot=1,\n",
    "    )\n",
    "    qID_datasets += [qID_dataset]\n",
    "raw_dataset = pd.concat(qID_datasets)\n",
    "training_dataset = prepare_df(raw_dataset.copy(), tokenizer).reset_index(drop=True)\n",
    "training_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5396b688-2568-4a9c-a9ca-cba4fc8776b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = training_dataset.sample(frac=1.0)\n",
    "len(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "778a8c38-4325-440f-8ea1-a5cf82099c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 18,436 || trainable model params: 0\n",
      "model params: 2,614,341,888 || trainable%: 0.0007051870332882797\n"
     ]
    }
   ],
   "source": [
    "# get reft model\n",
    "rank = 2\n",
    "layers = [10, 20]\n",
    "# position info about the interventions\n",
    "share_weights = True # whether the prefix and suffix interventions sharing weights.\n",
    "positions=\"f10+l10\"    # the intervening positions of prefix tokens (f[irst]1) and suffix tokens (l[ast]1).\n",
    "first_n, last_n = pyreft.parse_positions(positions)\n",
    "\n",
    "reft_config = pyreft.ReftConfig(representations=[{\n",
    "    \"layer\": layer, \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": rank,\n",
    "    \"intervention\": pyreft.DireftIntervention(embed_dim=model.config.hidden_size,\n",
    "    low_rank_dimension=rank)} for layer in layers])\n",
    "reft_model = pyreft.get_reft_model(model, reft_config)\n",
    "reft_model.set_device(\"cuda\")\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4098572a-c384-44cb-8d9a-a9344ade19c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:33, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.458600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.880400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.699600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.473500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './tmp/checkpoint-500/intervenable_model' already exists.\n"
     ]
    }
   ],
   "source": [
    "# prepare training data modules\n",
    "data_module = pyreft.make_multiple_position_supervised_data_module(\n",
    "    tokenizer, model, \n",
    "    [e for e in training_dataset.input],\n",
    "    [e for e in training_dataset.output], \n",
    "    positions=positions, num_interventions=len(reft_config.representations), share_weights=share_weights, nonstop=True)\n",
    "\n",
    "# train\n",
    "training_args = transformers.TrainingArguments(\n",
    "    num_train_epochs=100.0, output_dir=\"./tmp\", per_device_train_batch_size=10, \n",
    "    learning_rate=1e-3, logging_steps=40, report_to=[])\n",
    "trainer = pyreft.ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89bc5d68-163e-4c55-9110-4bd00e511233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: WHYNOTBIZF2G_W36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: A B A B A C C C A C A C A A C A A C A A C A C A C A C A C A A C C A\n",
      "Evaluating: GAP21Q33_r_W82\n",
      "Answer: C A B D A “ A A “ A B “ D ” A B “ D ” A “ A ” A “ A ” A “ A ” C C “\n",
      "Evaluating: NEIGHINTERA_W32\n",
      "Answer: E E E D C E G G E G C G C G E E A C E A D E D E D D E D E E D E D E\n",
      "Evaluating: FUTRCLASSc_W41\n",
      "Answer: B B B A A A D B B D B B B B B D D D D D D B B B B B B B B B B A B D\n",
      "Evaluating: TRAITPOLMF1B_W36\n",
      "Answer: B B B A B B C C C B B B A B A A C A C B B B B B A C B B C A C B A B\n",
      "Evaluating: FUD37A_W34\n",
      "Answer: A B E C C A A E C E A C tű A B C A A A A A A A B A A A E A C C C C C\n",
      "Evaluating: HIGHEDWRNGB_W36\n",
      "Answer: A B A B A B C A A A C javadoc A B A B A A A A A A B A B A B A A B A A A\n",
      "Evaluating: WHYNOTPOLF1C_W36\n",
      "Answer: A B C A B A B A C C C A A C A B C A A C A C C C C C A D A A A B A M\n",
      "Evaluating: GAP21Q4_f_W82\n",
      "Answer: A C A B D B B D B B B A B B A B B B B B A B D B B B C B B B B B D C\n",
      "Evaluating: ESSENPOLF1B_W36\n",
      "Answer: A B A B A B C A B A A A A A A B A A A B A A A A A A A C C A B A A A\n",
      "Evaluating: RQ4_F1Ba_W42\n",
      "Answer: B E E E E C E E E C E C E E E E E E A E C E C E E C宣伝 C E E E E A E\n",
      "Evaluating: RACESURV14_W43\n",
      "Answer: E E E A A B C C E C E E B C E C C C E E C E E C E C C C E A C E E A\n",
      "Evaluating: INFOCREATEa_W45\n",
      "Answer: B O A D D B D B D D D D D B B A D B D B B D B D B B D B D B B D B D\n",
      "Evaluating: GAP21Q19_a_W82\n",
      "Answer: B B E D B D B D B C B B D B D B D B D E B C B B B B B D B D B A B D\n",
      "Evaluating: GROWUPVIOL_W26\n",
      "Answer: B B B B B B B B B B B B B A B A B A B C B B B C B B B C B A C B B B\n",
      "Evaluating: FAMSURV23e_W50\n",
      "Answer: A B A B B B B A B B C B A B A A B B B A C B B B B B B B A B B A B A\n",
      "Evaluating: GUNTYPEOWNC_W26\n",
      "Answer: C C A D B C A D C D A C D D C D C A D D C D B C D B D D C D A C A D\n",
      "Evaluating: ROMRELDUR_W50\n",
      "Answer: F prov C, E, C, D, A, A, A, A, A, A, D, A, D, C, D, D,\n",
      "Answer: Focuk E A C D E C C C E G A G A F C C C C C D A C D D D D D D D D D D\n",
      "Evaluating: GAP21Q31_W82\n",
      "Answer: A B E A E A E A E D D E A A D D D Eliest D D D E E B A D D A D E B A D\n",
      "Evaluating: BILLION_W92\n",
      "Answer: C A B C B C B C B C B B N C B N C N A C N C A C B C B C A N N N C B\n",
      "Success rate: 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "def apply_chat_template(row):\n",
    "    messages = [{\"role\": \"user\", \"content\": row[\"input\"]}]\n",
    "    nobos = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)[1:]\n",
    "    return tokenizer.decode(nobos)\n",
    "\n",
    "test_pool = get_test_questions_with_distributions(\n",
    "    seen_qIDs={}, \n",
    "    demographic_group=demographic_group,\n",
    "    demographic=demographic,\n",
    ")\n",
    "test_qIDs = json.load(open(n_testing_qIDs))\n",
    "\n",
    "k = 1\n",
    "success_rates = []\n",
    "probabilities_list = []\n",
    "for test_qID in test_qIDs:\n",
    "    print(\"Evaluating:\", test_qID)\n",
    "    # test_qID = \"ECON5_d_W54\"\n",
    "    n = (sum(test_pool[test_qID][demographic].values()))\n",
    "    MC_options = list(test_pool[test_qID][demographic].keys())\n",
    "    all_options, probs = [], []\n",
    "    for i, option in enumerate(MC_options):\n",
    "        all_options.append(options[i])\n",
    "        probs.append(test_pool[test_qID][demographic][option]/n)\n",
    "    golden_dist = dict(zip(all_options, probs))\n",
    "    # print(\"Golden dist:\")\n",
    "    # print(golden_dist)\n",
    "\n",
    "    instruction = get_zeroshot_prompt_opinionqa(test_qID, output_type=\"sequence\")\n",
    "    \n",
    "    instruction = apply_chat_template({\"input\": instruction})\n",
    "    model_inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    unit_locations = torch.IntTensor([pyreft.get_intervention_locations(\n",
    "        last_position=model_inputs[\"input_ids\"].shape[-1], \n",
    "        first_n=first_n, \n",
    "        last_n=last_n,\n",
    "        pad_mode=\"last\",\n",
    "        num_interventions=len(reft_config.representations),\n",
    "        share_weights=share_weights\n",
    "    )]).permute(1, 0, 2).tolist()\n",
    "\n",
    "    successful_parsings = 0\n",
    "    total_attempts = 0\n",
    "    while successful_parsings < k:\n",
    "        _, outputs = reft_model.generate(\n",
    "            model_inputs, unit_locations={\"sources->base\": (None, unit_locations)},\n",
    "            intervene_on_prompt=True, max_new_tokens=36, do_sample=True, \n",
    "            eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0][model_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "        print(response)\n",
    "        success, result = parse_answers(response, all_options)\n",
    "        total_attempts += 1\n",
    "        if success:\n",
    "            successful_parsings += 1\n",
    "            probabilities_list.append([golden_dist, result[\"probabilities\"]])\n",
    "        success_rate = successful_parsings / total_attempts\n",
    "        success_rates += [success_rate]\n",
    "success_rate = np.array(success_rates).mean()\n",
    "print(\"Success rate:\", success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "94cfb411-188d-483e-bdb1-7e2138d0438c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.651281530069914"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = compute_l1_values(probabilities_list)\n",
    "json.dump(distances, open(\"distance_reft.json\", \"w\"))\n",
    "np.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6168f982-cb54-4a96-bdb5-75bfa7c68fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's dive into US politics! \n",
      "\n",
      "**It's a complex and constantly evolving system, so I'll break it down into a few key areas:** \n",
      "\n",
      "**1. Key Concepts:**\n",
      "\n",
      "* **Federalism:**  Power is divided between the federal (national) government and the states.\n",
      "* **Constitution:** The fundamental law of the land, outlining the basic structure of the government.\n",
      "* **Supremacy Clause:** The Constitution, federal laws made under its authority, and treaties are the highest law of the land. This can lead to tension, for example, between national and state laws.\n",
      "* **\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Tell me about US politics.\"\n",
    "\n",
    "instruction = apply_chat_template({\"input\": instruction})\n",
    "model_inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **model_inputs, max_new_tokens=128, do_sample=True, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "response = tokenizer.decode(outputs[0][model_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "580187f9-da6a-44c9-80bf-f17525232958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## US Politics: A Simplified Overview\n",
      "\n",
      "It's impossible to cover everything, but here are some key things to understand about US politics:\n",
      "\n",
      "**The Basics:**\n",
      "\n",
      "* **Federal Republic:** The US is a republic with a complex system of government: the President is the highest executive (but not the dictator) chosen by the Electoral College, the Congress is the legislative branch, and the Supreme Court is the judicial branch.\n",
      "* **Separation of Powers:** Power is divided among these three branches to prevent tyranny. Each branch has its own accountability mechanisms to ensure the others don't become too strong.\n",
      "* **Two-Party System\n"
     ]
    }
   ],
   "source": [
    "unit_locations = torch.IntTensor([pyreft.get_intervention_locations(\n",
    "    last_position=model_inputs[\"input_ids\"].shape[-1], \n",
    "    first_n=first_n, \n",
    "    last_n=last_n,\n",
    "    pad_mode=\"last\",\n",
    "    num_interventions=len(reft_config.representations),\n",
    "    share_weights=share_weights\n",
    ")]).permute(1, 0, 2).tolist()\n",
    "\n",
    "_, outputs = reft_model.generate(\n",
    "    model_inputs, unit_locations={\"sources->base\": (None, unit_locations)},\n",
    "    intervene_on_prompt=True, max_new_tokens=128, do_sample=True, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "response = tokenizer.decode(outputs[0][model_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0680ba-f2b4-4f65-a06c-404ab9275b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
