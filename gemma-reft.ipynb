{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ebb65a-ba2e-4234-8841-b2777b69472e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import pyreft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a8fd8d5-5e22-4ed0-92bc-fd70f70b41ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fc83136cc3495a9159b030d7eeb4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c0e45f6bf6490b96b1d4ccfeb19510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers, pyreft\n",
    "device = \"cuda\"\n",
    "\n",
    "model_name_or_path = \"google/gemma-2-2b-it\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=2048, \n",
    "    padding_side=\"right\", use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687947c9-cd53-4f7c-a028-f32e2bcc713c",
   "metadata": {},
   "source": [
    "### Specify your job parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d102207-f1ae-4a4f-a2cd-7c80b0ba5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_qIDs = 3         # We sample n q_IDs from the same demographic group for training.\n",
    "n_testing_qIDs = 10         # We sample unseen n q_IDs from the same demographic group for testing.\n",
    "\n",
    "# demographic group and output type\n",
    "demographic_group = \"POLPARTY\"\n",
    "demographic = \"Democrat\"\n",
    "output_type = \"sequence\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c7396-7dcb-405c-989a-075a076150e5",
   "metadata": {},
   "source": [
    "#### Getting a random qID and the corresponding training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b741c167-cd80-4cfa-8d58-15537c59c6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>qID</th>\n",
       "      <th>icl_qID</th>\n",
       "      <th>demographic_group</th>\n",
       "      <th>demographic</th>\n",
       "      <th>output_type</th>\n",
       "      <th>wave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nPlease simulate 30 sample...</td>\n",
       "      <td>Answer: E B C C C B E B A A B A C C A C A E B ...</td>\n",
       "      <td>GAP21Q33_c_W82</td>\n",
       "      <td>GOVPRIOkF2_W41</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nPlease simulate 30 sample...</td>\n",
       "      <td>Answer: A A B A A C A A A A B B A C A C A A A ...</td>\n",
       "      <td>GAP21Q33_c_W82</td>\n",
       "      <td>GOVPRIORITYd_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nPlease simulate 30 sample...</td>\n",
       "      <td>Answer: C D C C D C D C C C C D A D C C D C C ...</td>\n",
       "      <td>GAP21Q33_c_W82</td>\n",
       "      <td>GAP21Q33_q_W82</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <start_of_turn>user\\nPlease simulate 30 sample...   \n",
       "1  <start_of_turn>user\\nPlease simulate 30 sample...   \n",
       "2  <start_of_turn>user\\nPlease simulate 30 sample...   \n",
       "\n",
       "                                              output             qID  \\\n",
       "0  Answer: E B C C C B E B A A B A C C A C A E B ...  GAP21Q33_c_W82   \n",
       "1  Answer: A A B A A C A A A A B B A C A C A A A ...  GAP21Q33_c_W82   \n",
       "2  Answer: C D C C D C D C C C C D A D C C D C C ...  GAP21Q33_c_W82   \n",
       "\n",
       "            icl_qID demographic_group demographic output_type  \\\n",
       "0    GOVPRIOkF2_W41          POLPARTY    Democrat    sequence   \n",
       "1  GOVPRIORITYd_W54          POLPARTY    Democrat    sequence   \n",
       "2    GAP21Q33_q_W82          POLPARTY    Democrat    sequence   \n",
       "\n",
       "                                         wave  \n",
       "0  Pew_American_Trends_Panel_disagreement_100  \n",
       "1  Pew_American_Trends_Panel_disagreement_100  \n",
       "2  Pew_American_Trends_Panel_disagreement_100  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qIDs, waves = get_q_IDs_opinionqa()\n",
    "sampled_qIDs = random.sample(list(qIDs), n_training_qIDs)\n",
    "\n",
    "qID_datasets = []\n",
    "for qID in sampled_qIDs:\n",
    "    qID_dataset = get_few_shot_training_examples(\n",
    "        qID,\n",
    "        wave=\"Pew_American_Trends_Panel_disagreement_100\", \n",
    "        demographic_group=demographic_group,\n",
    "        demographic=demographic,\n",
    "        output_type=output_type, \n",
    "        dataset=\"opinionqa\",\n",
    "        n_shots=5,\n",
    "        n_simulations_per_shot=1,\n",
    "    )\n",
    "    qID_datasets += [qID_dataset]\n",
    "raw_dataset = pd.concat(qID_datasets)\n",
    "training_dataset = prepare_df(raw_dataset.copy(), tokenizer).reset_index(drop=True)\n",
    "training_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778a8c38-4325-440f-8ea1-a5cf82099c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 18,436 || trainable model params: 0\n",
      "model params: 2,614,341,888 || trainable%: 0.0007051870332882797\n"
     ]
    }
   ],
   "source": [
    "# get reft model\n",
    "reft_config = pyreft.ReftConfig(representations={\n",
    "    \"layer\": 20, \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 4,\n",
    "    \"intervention\": pyreft.LoreftIntervention(embed_dim=model.config.hidden_size,\n",
    "    low_rank_dimension=4)})\n",
    "reft_model = pyreft.get_reft_model(model, reft_config)\n",
    "reft_model.set_device(\"cuda\")\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4098572a-c384-44cb-8d9a-a9344ade19c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:26, Epoch 60/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.437700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './tmp/checkpoint-120/intervenable_model' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# prepare training data modules\n",
    "data_module = pyreft.make_last_position_supervised_data_module(\n",
    "    tokenizer, model, [input for input in training_dataset.input], \n",
    "    [output for output in training_dataset.output], nonstop=False)\n",
    "\n",
    "# train\n",
    "training_args = transformers.TrainingArguments(\n",
    "    num_train_epochs=60.0, output_dir=\"./tmp\", per_device_train_batch_size=8, \n",
    "    learning_rate=9e-3, logging_steps=40, report_to=[])\n",
    "trainer = pyreft.ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc5d68-163e-4c55-9110-4bd00e511233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(row):\n",
    "    messages = [{\"role\": \"user\", \"content\": row[\"input\"]}]\n",
    "    nobos = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)[1:]\n",
    "    return tokenizer.decode(nobos)\n",
    "\n",
    "test_pool = get_test_questions_with_distributions(\n",
    "    seen_qIDs=set(training_dataset.qID).union(training_dataset.icl_qID)\n",
    ")\n",
    "test_qIDs = random.sample(test_pool.keys(), n_testing_qIDs)\n",
    "\n",
    "k = 10\n",
    "success_rates = []\n",
    "probabilities_list = []\n",
    "for test_qID in test_qIDs:\n",
    "    print(\"Evaluating:\", test_qID)\n",
    "    # test_qID = \"ECON5_d_W54\"\n",
    "    print(\"testing qID:\", test_qID)\n",
    "    n = (sum(test_pool[test_qID][demographic].values()))\n",
    "    MC_options = list(test_pool[test_qID][demographic].keys())\n",
    "    all_options, probs = [], []\n",
    "    for i, option in enumerate(MC_options):\n",
    "        all_options.append(options[i])\n",
    "        probs.append(test_pool[test_qID][demographic][option]/n)\n",
    "    golden_dist = dict(zip(all_options, probs))\n",
    "    # print(\"Golden dist:\")\n",
    "    # print(golden_dist)\n",
    "\n",
    "    instruction = get_zeroshot_prompt_opinionqa(test_qID, output_type=\"sequence\")\n",
    "    \n",
    "    instruction = apply_chat_template({\"input\": instruction})\n",
    "    model_inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
    "    base_unit_location = model_inputs[\"input_ids\"].shape[-1] - 1  # last position\n",
    "\n",
    "    successful_parsings = 0\n",
    "    total_attempts = 0\n",
    "    while successful_parsings < k:\n",
    "        _, outputs = reft_model.generate(\n",
    "            model_inputs, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "            intervene_on_prompt=True, max_new_tokens=36, do_sample=True, \n",
    "            eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0][base_unit_location+1:], skip_special_tokens=True)\n",
    "        # print(response)\n",
    "        success, result = parse_answers(response, all_options)\n",
    "        total_attempts += 1\n",
    "        if success:\n",
    "            successful_parsings += 1\n",
    "            probabilities_list.append([golden_dist, result[\"probabilities\"]])\n",
    "        success_rate = successful_parsings / total_attempts\n",
    "        success_rates += [success_rate]\n",
    "success_rate = np.array(success_rates).mean()\n",
    "print(\"Success rate:\", success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94cfb411-188d-483e-bdb1-7e2138d0438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsds = compute_jsd_values(probabilities_list)\n",
    "json.dump(jsds, open(\"jsds_reft.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
