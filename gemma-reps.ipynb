{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9530ef-f5de-477f-9a2d-0feb1c7e0c00",
   "metadata": {},
   "source": [
    "#### All these are just Diff-In-Mean Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abbe9142-c816-48cc-880e-12d8d2924ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import torch, transformers, datasets, einops\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence, Union, List, Any\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from pyvene import (\n",
    "    SourcelessIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    "    CollectIntervention,\n",
    "    InterventionOutput\n",
    ")\n",
    "from pyvene import (\n",
    "    IntervenableConfig,\n",
    "    IntervenableModel\n",
    ")\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def set_decoder_norm_to_unit_norm(model):\n",
    "    assert model.proj.weight is not None, \"Decoder weight was not initialized.\"\n",
    "\n",
    "    eps = torch.finfo(model.proj.weight.dtype).eps\n",
    "    norm = torch.norm(model.proj.weight.data, dim=1, keepdim=True)\n",
    "    model.proj.weight.data /= norm + eps\n",
    "\n",
    "def gather_residual_activations(model, target_layer, inputs):\n",
    "  target_act = None\n",
    "  def gather_target_act_hook(mod, inputs, outputs):\n",
    "    nonlocal target_act # make sure we can modify the target_act from the outer scope\n",
    "    target_act = outputs[0]\n",
    "    return outputs\n",
    "  handle = model.model.layers[target_layer].register_forward_hook(\n",
    "      gather_target_act_hook, always_call=True)\n",
    "  _ = model.forward(**inputs)\n",
    "  handle.remove()\n",
    "  return target_act\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, low_rank_dimension):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # Linear layer: input_dim -> 1 output (since binary classification)\n",
    "        self.proj = torch.nn.Linear(input_dim, low_rank_dimension)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "@dataclass\n",
    "class DataCollator(object):\n",
    "    \"\"\"Collate examples for ReFT.\"\"\"\n",
    "    \n",
    "    tokenizer: transformers.AutoTokenizer\n",
    "    data_collator: transformers.DataCollator\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        max_intervention_len = max([len(inst[\"intervention_locations\"][0]) for inst in instances])\n",
    "        max_seq_len = max([len(inst[\"input_ids\"]) for inst in instances])\n",
    "        \n",
    "        for inst in instances:\n",
    "            non_pad_len = len(inst[\"input_ids\"])\n",
    "\n",
    "            _intervention_mask = torch.ones_like(inst[\"intervention_locations\"][0])\n",
    "            _intervention_location_paddings = torch.tensor(\n",
    "                [[len(inst[\"input_ids\"]) for _ in range(max_intervention_len - len(inst[\"intervention_locations\"][0]))]])\n",
    "            _intervention_mask_paddings = torch.tensor(\n",
    "                [0 for _ in range(max_intervention_len - len(inst[\"intervention_locations\"][0]))])\n",
    "            inst[\"intervention_locations\"] = torch.cat([inst[\"intervention_locations\"], _intervention_location_paddings], dim=-1).int()\n",
    "            inst[\"intervention_masks\"] = torch.cat([_intervention_mask, _intervention_mask_paddings], dim=-1).int()\n",
    "\n",
    "            _input_id_paddings = torch.tensor(\n",
    "                [self.tokenizer.pad_token_id for _ in range(max_seq_len - non_pad_len)])\n",
    "            inst[\"input_ids\"] = torch.cat((inst[\"input_ids\"], torch.tensor([self.tokenizer.pad_token_id]), _input_id_paddings)).int()\n",
    "            inst[\"attention_mask\"] = (inst[\"input_ids\"] != self.tokenizer.pad_token_id).int()\n",
    "            inst[\"labels\"] = inst[\"labels\"].int()\n",
    "        batch_inputs = self.data_collator(instances)\n",
    "        return batch_inputs\n",
    "\n",
    "def make_data_module(\n",
    "    tokenizer: transformers.PreTrainedTokenizer, model, df, prefix_length=1\n",
    "):\n",
    "    all_input_ids, all_labels, all_intervention_locations = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        input_ids = tokenizer(\n",
    "            row[\"input\"]+row[\"output\"], max_length=1024, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        base_length = len(input_ids)\n",
    "        intervention_locations = torch.tensor([[i for i in range(prefix_length, base_length)]])\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_labels.append(row[\"labels\"])\n",
    "        all_intervention_locations.append(intervention_locations)\n",
    "\n",
    "    train_dataset = datasets.Dataset.from_dict({\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"labels\": all_labels,\n",
    "        \"intervention_locations\": all_intervention_locations\n",
    "    })\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'labels', 'intervention_locations'])\n",
    "\n",
    "    data_collator_fn = transformers.DefaultDataCollator(\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    data_collator = DataCollator(tokenizer=tokenizer, data_collator=data_collator_fn)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n",
    "\n",
    "def make_dataloader(tokenizer, model, examples):\n",
    "    data_module = make_data_module(tokenizer, model, examples)\n",
    "    train_dataloader = DataLoader(\n",
    "        data_module[\"train_dataset\"], \n",
    "        shuffle=True, batch_size=8, \n",
    "        collate_fn=data_module[\"data_collator\"])\n",
    "    return train_dataloader\n",
    "\n",
    "@torch.no_grad()\n",
    "def train(tokenizer, model, ax, examples, layer, prefix_length=4):\n",
    "    train_dataloader = make_dataloader(tokenizer, model, examples)\n",
    "    torch.cuda.empty_cache()\n",
    "    ax.eval()\n",
    "    # Main training loop.\n",
    "    positive_activations = []\n",
    "    negative_activations = []\n",
    "    for batch in train_dataloader:\n",
    "        # prepare input\n",
    "        inputs = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        activations = gather_residual_activations(\n",
    "            model, layer, \n",
    "            {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]}\n",
    "        ).detach()\n",
    "        nonbos_mask = inputs[\"attention_mask\"][:,prefix_length:]\n",
    "        activations = activations[:,prefix_length:][nonbos_mask.bool()]\n",
    "        labels = inputs[\"labels\"].unsqueeze(1).repeat(\n",
    "            1, inputs[\"input_ids\"].shape[1] - prefix_length)\n",
    "        positive_activations.append(activations[labels[nonbos_mask.bool()] == 1])\n",
    "        negative_activations.append(activations[labels[nonbos_mask.bool()] != 1])\n",
    "\n",
    "    mean_positive_activation = torch.cat(positive_activations, dim=0).mean(dim=0)\n",
    "    mean_negative_activation = torch.cat(negative_activations, dim=0).mean(dim=0)\n",
    "    ax.proj.weight.data = mean_positive_activation.unsqueeze(0) - mean_negative_activation.unsqueeze(0)\n",
    "    set_decoder_norm_to_unit_norm(ax)\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "def get_logits(tokenizer, model, ax, concept_id=0, k=10):\n",
    "    top_logits, neg_logits = [None], [None]\n",
    "    if concept_id is not None:\n",
    "        W_U = model.lm_head.weight.T\n",
    "        W_U = W_U * (model.model.norm.weight +\n",
    "                    torch.ones_like(model.model.norm.weight))[:, None]\n",
    "        W_U -= einops.reduce(\n",
    "            W_U, \"d_model d_vocab -> 1 d_vocab\", \"mean\"\n",
    "        )\n",
    "\n",
    "        vocab_logits = ax.proj.weight.data[concept_id] @ W_U\n",
    "        top_values, top_indices = vocab_logits.topk(k=k, sorted=True)\n",
    "        top_tokens = tokenizer.batch_decode(top_indices.unsqueeze(dim=-1))\n",
    "        top_logits = [list(zip(top_tokens, top_values.tolist()))]\n",
    "        \n",
    "        neg_values, neg_indices = vocab_logits.topk(k=k, largest=False, sorted=True)\n",
    "        neg_tokens = tokenizer.batch_decode(neg_indices.unsqueeze(dim=-1))\n",
    "        neg_logits = [list(zip(neg_tokens, neg_values.tolist()))]\n",
    "    return top_logits, neg_logits\n",
    "\n",
    "def get_few_shot_contrastive_inputs(\n",
    "    q_ID,\n",
    "    wave=\"Pew_American_Trends_Panel_disagreement_100\", \n",
    "    demographic_group=\"POLPARTY\",\n",
    "    demographic=\"Democrat\",\n",
    "    output_type=\"model_logprobs\",\n",
    "    n_shots=5,\n",
    "    n_simulations_per_shot=1,\n",
    "    provide_ground_truth_distribution=False,\n",
    "):\n",
    "    data_path = '{}/opinions_qa/data/human_resp/'.format(os.getcwd())\n",
    "    demographic_in_prompt = demographic\n",
    "    data = json.load(open(data_path + wave + '/' + demographic_group + \"_data.json\"))\n",
    "    prompt = \"Your task is to simulate an answer to a new question. \"\n",
    "\n",
    "    if output_type=='sequence':\n",
    "        prompt+= 'After the examples, please simulate 30 samples for the new question asked. Please only respond with 30 multiple choice answers, no extra spaces, characters, quotes or text. Please only produce 30 characters. Answers with more than 30 characters will not be accepted.'\n",
    "    elif output_type=='model_logprobs': \n",
    "        prompt += 'After the examples, please simulate an answer for the question asked. Please only respond with a single multiple choice answer, no extra spaces, characters, quotes or text. Please only produce 1 character. Answers with more than one characters will not be accepted.'\n",
    "    elif output_type=='express_distribution': \n",
    "        prompt += 'After the examples, please express the distribution of answers for the question asked. Please only respond in the exact format of a dictionary mapping answer choice letter to probability, no extra spaces, characters, quotes or text. Please only produce 1 sentence in this format. Answers outside of this format will not be accepted.'\n",
    "\n",
    "    # we need the larger set to get icl demos\n",
    "    if wave == 'Pew_American_Trends_Panel_disagreement_100':\n",
    "        icl_wave='Pew_American_Trends_Panel_disagreement_500'\n",
    "    icl_data = json.load(open(data_path + icl_wave + '/' + demographic_group + \"_data.json\"))\n",
    "\n",
    "    # get icl qids\n",
    "    ICL_qIDS = get_ICL_qIDs(\n",
    "        q_ID=q_ID, wave=icl_wave, \n",
    "        demographic_group=demographic_group, demographic=demographic)\n",
    "\n",
    "    examples = []\n",
    "    \n",
    "    for icl_qID in ICL_qIDS[:n_shots]:\n",
    "        if icl_qID == q_ID:\n",
    "            continue\n",
    "\n",
    "        n = (sum(icl_data[icl_qID][demographic].values()))\n",
    "        MC_options = list(icl_data[icl_qID][demographic].keys())\n",
    "        all_options, probs = [], []\n",
    "        for i, option in enumerate(MC_options):\n",
    "            all_options.append(options[i])\n",
    "            probs.append(icl_data[icl_qID][demographic][option]/n)\n",
    "            if provide_ground_truth_distribution:\n",
    "                prompt +=\"{} be {}%, \".format(option, int((icl_data[icl_qID][demographic][option]/n)*100))\n",
    "\n",
    "        example_input = prompt + \"\\nQuestion: \" + icl_data[icl_qID]['question_text'] + \"?\\n\"\n",
    "        for i, option in enumerate(MC_options):\n",
    "            example_input +=\"{}. {}. \".format(options[i], option)\n",
    "        for _ in range(n_simulations_per_shot):\n",
    "            examples.append([example_input, q_ID, icl_qID, demographic_group, demographic, output_type, wave])\n",
    "\n",
    "    return pd.DataFrame(examples, columns=[\n",
    "        'input', 'qID', 'icl_qID', 'demographic_group', 'demographic', 'output_type', 'wave'\n",
    "    ])\n",
    "\n",
    "class AdditionIntervention(\n",
    "    SourcelessIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        # Note that we initialise these to zeros because we're loading in pre-trained weights.\n",
    "        # If you want to train your own SAEs then we recommend using blah\n",
    "        super().__init__(**kwargs, keep_last_dim=True)\n",
    "        self.proj = torch.nn.Linear(\n",
    "                self.embed_dim, kwargs[\"low_rank_dimension\"], bias=True)\n",
    "\n",
    "    def forward(self, base, source=None, subspaces=None):\n",
    "        # use subspaces[\"idx\"] to select the correct weight vector\n",
    "        steering_vec = subspaces[\"max_act\"] * subspaces[\"mag\"] * self.proj.weight[subspaces[\"idx\"]].unsqueeze(dim=0)\n",
    "        output = base + steering_vec.unsqueeze(dim=1)\n",
    "        return output\n",
    "        \n",
    "class SubspaceIntervention(\n",
    "    SourcelessIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs, keep_last_dim=True)\n",
    "        self.proj = torch.nn.Linear(\n",
    "            self.embed_dim, kwargs[\"low_rank_dimension\"], bias=True)\n",
    "    \n",
    "    def forward(self, base, source=None, subspaces=None):\n",
    "        v = self.proj.weight[[subspaces[\"idx\"]]*base.shape[0]].unsqueeze(dim=-1) # bs, h, 1\n",
    "        # get orthogonal component\n",
    "        latent = torch.relu(torch.bmm(base, v)) # bs, s, 1\n",
    "        proj_vec = torch.bmm(latent, v.permute(0, 2, 1)) # bs, s, 1 * bs, 1, h = bs, s, h\n",
    "        base_orthogonal = base - proj_vec\n",
    "\n",
    "        steering_scale = subspaces[\"max_act\"] * subspaces[\"mag\"]\n",
    "        steering_vec = steering_scale * v.permute(0, 2, 1) # bs, 1, h\n",
    "        \n",
    "        # Replace the projection component with the steering vector\n",
    "        output = base_orthogonal + steering_vec \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf2a67c-9281-4f2b-a58a-e45ab01834e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f834881572ea497fbdc8633c3bea72b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d3a4b8cb10472e938ea1f9a3cf3b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model_name_or_path = \"google/gemma-2-2b-it\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=2048, \n",
    "    padding_side=\"right\", use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416368eb-746a-446b-b697-571c39b9967a",
   "metadata": {},
   "source": [
    "### Specify your job parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26da8f7-96d2-4475-bc9f-72d281634eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run random baseline to get these files first!\n",
    "n_training_qIDs = \"train_qIDs.json\"\n",
    "n_testing_qIDs = \"test_qIDs.json\"\n",
    "\n",
    "# demographic group and output type\n",
    "demographic_group = \"POLPARTY\"\n",
    "demographic = \"Republican\"\n",
    "output_type = \"sequence\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ef0d1-7148-4460-822c-8d2ca21f1fbc",
   "metadata": {},
   "source": [
    "#### Training and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ca9586-fdb7-47bf-9d22-c7b1193a171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/juice2/scr2/wuzhengx/cs329h/Benchmarking-Distributional-Alignment-of-Small-Language-Models/utils.py:138: RuntimeWarning: invalid value encountered in divide\n",
      "  icl_values = np.array(icl_values)/np.sum(icl_values)\n",
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def apply_chat_template(row):\n",
    "    messages = [{\"role\": \"user\", \"content\": row[\"input\"]}]\n",
    "    nobos = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)[1:]\n",
    "    return tokenizer.decode(nobos)\n",
    "\n",
    "sampled_qIDs = json.load(open(n_training_qIDs))\n",
    "\n",
    "qID_datasets = []\n",
    "for qID in sampled_qIDs:\n",
    "    qID_dataset = get_few_shot_training_examples(\n",
    "        qID,\n",
    "        wave=\"Pew_American_Trends_Panel_disagreement_100\", \n",
    "        demographic_group=demographic_group,\n",
    "        demographic=demographic,\n",
    "        output_type=\"model_logprobs\", \n",
    "        dataset=\"opinionqa\",\n",
    "        n_shots=5,\n",
    "        n_simulations_per_shot=1,\n",
    "    )\n",
    "    qID_datasets += [qID_dataset]\n",
    "raw_dataset = pd.concat(qID_datasets)\n",
    "training_dataset = prepare_df(raw_dataset.copy(), tokenizer).reset_index(drop=True)\n",
    "\n",
    "qID_c_datasets = []\n",
    "for qID in sampled_qIDs:\n",
    "    contrastive_df = get_few_shot_contrastive_inputs(\n",
    "        qID,\n",
    "        wave=\"Pew_American_Trends_Panel_disagreement_100\", \n",
    "        demographic_group=demographic_group,\n",
    "        demographic=demographic,\n",
    "        output_type=\"model_logprobs\", \n",
    "        n_shots=5,\n",
    "        n_simulations_per_shot=1,\n",
    "    )\n",
    "    qID_c_datasets += [contrastive_df]\n",
    "raw_c_dataset = pd.concat(qID_c_datasets)\n",
    "\n",
    "all_responses = []\n",
    "for index, row in raw_c_dataset.iterrows():\n",
    "    instruction = apply_chat_template({\"input\": row[\"input\"]})\n",
    "    prompt = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
    "    model_response = model.generate(\n",
    "        **prompt, max_new_tokens=3, do_sample=True, \n",
    "        eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    "    )\n",
    "    response = tokenizer.decode(model_response[0][prompt[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    all_responses += [response.strip()]\n",
    "\n",
    "raw_c_dataset[\"output\"] = all_responses\n",
    "contrastive_dataset = prepare_df(raw_c_dataset.copy(), tokenizer)\n",
    "training_dataset[\"labels\"] = 1\n",
    "contrastive_dataset[\"labels\"] = 0\n",
    "contrastive_training_dataset = pd.concat([training_dataset, contrastive_dataset]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6949c1be-5d41-446c-8e8a-685234841877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>qID</th>\n",
       "      <th>icl_qID</th>\n",
       "      <th>demographic_group</th>\n",
       "      <th>demographic</th>\n",
       "      <th>output_type</th>\n",
       "      <th>wave</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nPlease simulate an answer...</td>\n",
       "      <td>Answer: C</td>\n",
       "      <td>SOCIETY_SSM_W92</td>\n",
       "      <td>GAYMARR2_W32</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>model_logprobs</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nPlease simulate an answer...</td>\n",
       "      <td>Answer: C</td>\n",
       "      <td>SOCIETY_SSM_W92</td>\n",
       "      <td>FAMSURV6_W50</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>model_logprobs</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nPlease simulate an answer...</td>\n",
       "      <td>Answer: F</td>\n",
       "      <td>SOCIETY_SSM_W92</td>\n",
       "      <td>SOCIETY_RHIST_W92</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>model_logprobs</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nPlease simulate an answer...</td>\n",
       "      <td>Answer: F</td>\n",
       "      <td>SOCIETY_SSM_W92</td>\n",
       "      <td>SOCIETY_TRANS_W92</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>model_logprobs</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nPlease simulate an answer...</td>\n",
       "      <td>Answer: A</td>\n",
       "      <td>SOCIETY_SSM_W92</td>\n",
       "      <td>SOCIETY_WHT_W92</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>model_logprobs</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>C.</td>\n",
       "      <td>RACESURV34a_W43</td>\n",
       "      <td>WHADVANT_W32</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>model_logprobs</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>C</td>\n",
       "      <td>RACESURV34a_W43</td>\n",
       "      <td>WHADVANT_W92</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>model_logprobs</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>D</td>\n",
       "      <td>RACESURV34a_W43</td>\n",
       "      <td>RACESURV5b_W43</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>model_logprobs</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>F</td>\n",
       "      <td>RACESURV34a_W43</td>\n",
       "      <td>RACESURV5d_W43</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>model_logprobs</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>F</td>\n",
       "      <td>RACESURV34a_W43</td>\n",
       "      <td>RACESURV5e_W43</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Republican</td>\n",
       "      <td>model_logprobs</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input     output  \\\n",
       "0   <start_of_turn>user\\nPlease simulate an answer...  Answer: C   \n",
       "1   <start_of_turn>user\\nPlease simulate an answer...  Answer: C   \n",
       "2   <start_of_turn>user\\nPlease simulate an answer...  Answer: F   \n",
       "3   <start_of_turn>user\\nPlease simulate an answer...  Answer: F   \n",
       "4   <start_of_turn>user\\nPlease simulate an answer...  Answer: A   \n",
       "..                                                ...        ...   \n",
       "95  <start_of_turn>user\\nYour task is to simulate ...         C.   \n",
       "96  <start_of_turn>user\\nYour task is to simulate ...          C   \n",
       "97  <start_of_turn>user\\nYour task is to simulate ...          D   \n",
       "98  <start_of_turn>user\\nYour task is to simulate ...          F   \n",
       "99  <start_of_turn>user\\nYour task is to simulate ...          F   \n",
       "\n",
       "                qID            icl_qID demographic_group demographic  \\\n",
       "0   SOCIETY_SSM_W92       GAYMARR2_W32          POLPARTY  Republican   \n",
       "1   SOCIETY_SSM_W92       FAMSURV6_W50          POLPARTY  Republican   \n",
       "2   SOCIETY_SSM_W92  SOCIETY_RHIST_W92          POLPARTY  Republican   \n",
       "3   SOCIETY_SSM_W92  SOCIETY_TRANS_W92          POLPARTY  Republican   \n",
       "4   SOCIETY_SSM_W92    SOCIETY_WHT_W92          POLPARTY  Republican   \n",
       "..              ...                ...               ...         ...   \n",
       "95  RACESURV34a_W43       WHADVANT_W32          POLPARTY  Republican   \n",
       "96  RACESURV34a_W43       WHADVANT_W92          POLPARTY  Republican   \n",
       "97  RACESURV34a_W43     RACESURV5b_W43          POLPARTY  Republican   \n",
       "98  RACESURV34a_W43     RACESURV5d_W43          POLPARTY  Republican   \n",
       "99  RACESURV34a_W43     RACESURV5e_W43          POLPARTY  Republican   \n",
       "\n",
       "       output_type                                        wave  labels  \n",
       "0   model_logprobs  Pew_American_Trends_Panel_disagreement_100       1  \n",
       "1   model_logprobs  Pew_American_Trends_Panel_disagreement_100       1  \n",
       "2   model_logprobs  Pew_American_Trends_Panel_disagreement_100       1  \n",
       "3   model_logprobs  Pew_American_Trends_Panel_disagreement_100       1  \n",
       "4   model_logprobs  Pew_American_Trends_Panel_disagreement_100       1  \n",
       "..             ...                                         ...     ...  \n",
       "95  model_logprobs  Pew_American_Trends_Panel_disagreement_100       0  \n",
       "96  model_logprobs  Pew_American_Trends_Panel_disagreement_100       0  \n",
       "97  model_logprobs  Pew_American_Trends_Panel_disagreement_100       0  \n",
       "98  model_logprobs  Pew_American_Trends_Panel_disagreement_100       0  \n",
       "99  model_logprobs  Pew_American_Trends_Panel_disagreement_100       0  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc39f76b-100a-4446-9bd0-867a7a49597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "LAYER = 20\n",
    "\n",
    "ax = LogisticRegressionModel(model.config.hidden_size, 1)\n",
    "ax = ax.to(DEVICE)\n",
    "train(tokenizer, model, ax, contrastive_training_dataset, LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf05f05-58ef-4834-966e-09a11d9ed993",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_ax = AdditionIntervention(\n",
    "    embed_dim=model.config.hidden_size, \n",
    "    low_rank_dimension=1,\n",
    ")\n",
    "steering_ax.proj.weight.data = ax.proj.weight.data\n",
    "steering_config = IntervenableConfig(representations=[{\n",
    "    \"layer\": LAYER,\n",
    "    \"component\": f\"model.layers[{LAYER}].output\",\n",
    "    \"low_rank_dimension\": 1,\n",
    "    \"intervention\": steering_ax} for l in [LAYER]])\n",
    "steering_model = IntervenableModel(steering_config, model)\n",
    "steering_model.set_device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56e4a819-9d75-49f7-83cb-fab0e2931ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: WHYNOTBIZF2G_W36\n",
      "Evaluating: GAP21Q33_r_W82\n",
      "Evaluating: NEIGHINTERA_W32\n",
      "Evaluating: FUTRCLASSc_W41\n",
      "Evaluating: TRAITPOLMF1B_W36\n",
      "Evaluating: FUD37A_W34\n",
      "Evaluating: HIGHEDWRNGB_W36\n",
      "Evaluating: WHYNOTPOLF1C_W36\n",
      "Evaluating: GAP21Q4_f_W82\n",
      "Evaluating: ESSENPOLF1B_W36\n",
      "Evaluating: RQ4_F1Ba_W42\n",
      "Evaluating: RACESURV14_W43\n",
      "Evaluating: INFOCREATEa_W45\n",
      "Evaluating: GAP21Q19_a_W82\n",
      "Evaluating: GROWUPVIOL_W26\n",
      "Evaluating: FAMSURV23e_W50\n",
      "Evaluating: GUNTYPEOWNC_W26\n",
      "Evaluating: ROMRELDUR_W50\n",
      "Evaluating: GAP21Q31_W82\n",
      "Evaluating: BILLION_W92\n",
      "Success rate: 0.3870614035087719\n"
     ]
    }
   ],
   "source": [
    "def apply_chat_template(row):\n",
    "    messages = [{\"role\": \"user\", \"content\": row[\"input\"]}]\n",
    "    nobos = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)[1:]\n",
    "    return tokenizer.decode(nobos)\n",
    "\n",
    "test_pool = get_test_questions_with_distributions(\n",
    "    seen_qIDs={}\n",
    ")\n",
    "test_qIDs = json.load(open(n_testing_qIDs))\n",
    "\n",
    "k = 1\n",
    "success_rates = []\n",
    "probabilities_list = []\n",
    "for test_qID in test_qIDs:\n",
    "    print(\"Evaluating:\", test_qID)\n",
    "    # test_qID = \"ECON5_d_W54\"\n",
    "    n = (sum(test_pool[test_qID][demographic].values()))\n",
    "    MC_options = list(test_pool[test_qID][demographic].keys())\n",
    "    all_options, probs = [], []\n",
    "    for i, option in enumerate(MC_options):\n",
    "        all_options.append(options[i])\n",
    "        probs.append(test_pool[test_qID][demographic][option]/n)\n",
    "    golden_dist = dict(zip(all_options, probs))\n",
    "    # print(\"Golden dist:\")\n",
    "    # print(golden_dist)\n",
    "\n",
    "    instruction = get_zeroshot_prompt_opinionqa(test_qID, output_type=\"sequence\")\n",
    "    instruction = apply_chat_template({\"input\": instruction})\n",
    "    # print(instruction)\n",
    "    model_inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    successful_parsings = 0\n",
    "    total_attempts = 0\n",
    "    while successful_parsings < k:\n",
    "        _, outputs = steering_model.generate(\n",
    "            model_inputs, unit_locations=None, intervene_on_prompt=True, \n",
    "            subspaces=[{\"mag\": 0.8, \"max_act\": 100, \"idx\": 0}], max_new_tokens=32, do_sample=True, \n",
    "            eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0][model_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "        # print(response)\n",
    "        success, result = parse_answers(response, all_options, answer_tag=False)\n",
    "        total_attempts += 1\n",
    "        if success:\n",
    "            successful_parsings += 1\n",
    "            probabilities_list.append([golden_dist, result[\"probabilities\"]])\n",
    "        success_rate = successful_parsings / total_attempts\n",
    "        success_rates += [success_rate]\n",
    "success_rate = np.array(success_rates).mean()\n",
    "print(\"Success rate:\", success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf9008ff-94c4-4029-ab40-0e4a33173e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6895270637451534"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = compute_l1_values(probabilities_list)\n",
    "json.dump(distances, open(\"distance_reps.json\", \"w\"))\n",
    "np.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd5a0cf7-955b-437a-be31-df3381b9e652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[(' conservative', 0.78515625),\n",
       "   ('kenstock', 0.77734375),\n",
       "   (' Republicans', 0.7578125),\n",
       "   (' FetchType', 0.75),\n",
       "   (' GOP', 0.71484375),\n",
       "   (' conservatives', 0.703125),\n",
       "   (' Conservative', 0.6875),\n",
       "   (' Republican', 0.6875),\n",
       "   ('Conservative', 0.6875),\n",
       "   (' Trump', 0.65625)]],\n",
       " [[(' CreateTagHelper', -0.66015625),\n",
       "   ('AddTagHelper', -0.64453125),\n",
       "   (' AssemblyCulture', -0.63671875),\n",
       "   ('enumi', -0.6328125),\n",
       "   ('EndInit', -0.58203125),\n",
       "   (' utafitiHapana', -0.56640625),\n",
       "   ('MLLoader', -0.53515625),\n",
       "   ('Tembelea', -0.51171875),\n",
       "   ('BeginContext', -0.48828125),\n",
       "   (' EconPapers', -0.48828125)]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_logits(tokenizer, model, ax, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dea3e165-b683-4600-84f3-9590a3d88124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The **Democratic Party** is a popular political party in the U.S. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Do not show bias towards any political party. Name a popular political party in the U.S.\"\n",
    "\n",
    "instruction = apply_chat_template({\"input\": instruction})\n",
    "model_inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **model_inputs, max_new_tokens=128, do_sample=True, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "response = tokenizer.decode(outputs[0][model_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59d12cfd-61e1-4b25-b657-901df024f774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Republican Party is a prominent and influential political party in the United States.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, outputs = steering_model.generate(\n",
    "    model_inputs, unit_locations=None, intervene_on_prompt=True, \n",
    "    subspaces=[{\"mag\": 2.0, \"max_act\": 100, \"idx\": 0}], max_new_tokens=128, do_sample=True, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "response = tokenizer.decode(outputs[0][model_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046b104-40de-4bdf-9120-828f8c87c814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ab85d-6959-44d4-9698-b5f8b13272d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
