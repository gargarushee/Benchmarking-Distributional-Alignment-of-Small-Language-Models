{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "abbe9142-c816-48cc-880e-12d8d2924ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import torch, transformers, datasets, einops\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence, Union, List, Any\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from pyvene import (\n",
    "    SourcelessIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    "    CollectIntervention,\n",
    "    InterventionOutput\n",
    ")\n",
    "from pyvene import (\n",
    "    IntervenableConfig,\n",
    "    IntervenableModel\n",
    ")\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "def apply_chat_template(row):\n",
    "    messages = [{\"role\": \"user\", \"content\": row[\"input\"]}]\n",
    "    nobos = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)[1:]\n",
    "    return tokenizer.decode(nobos)\n",
    "\n",
    "def prepare_df(original_df, tokenizer):\n",
    "    original_df['input'] = original_df.apply(apply_chat_template, axis=1)\n",
    "    return original_df # do nothing, the task will be standard instruction tuning.\n",
    "\n",
    "def apply_zeroshot_prompt_template(\n",
    "    qID,\n",
    "    wave=\"Pew_American_Trends_Panel_disagreement_500\", \n",
    "    demographic_group=\"POLPARTY\",\n",
    "    demographic=\"Democrat\",\n",
    "    output_type=\"model_logprobs\",\n",
    "    provide_ground_truth_distribution=False\n",
    "):\n",
    "    data_path = '{}/opinions_qa/data/human_resp/'.format(os.getcwd())\n",
    "    demographic_in_prompt = demographic\n",
    "    data = json.load(open(data_path + wave + '/' + demographic_group + \"_data.json\"))\n",
    "    prompt = \"Your task is to simulate an answer to a new question from the group of {}s. \".format(demographic_in_prompt, demographic_in_prompt)\n",
    "\n",
    "    if output_type=='sequence':\n",
    "        prompt+= 'After the examples, please simulate 30 samples from a group of {} for the new question asked. Please only respond with 30 multiple choice answers, no extra spaces, characters, quotes or text. Please only produce 30 characters. Answers with more than 30 characters will not be accepted.'.format(demographic_in_prompt)\n",
    "    elif output_type=='model_logprobs': \n",
    "        prompt += 'After the examples, please simulate an answer from a group of \"{}\" for the question asked. Please only respond with a single multiple choice answer, no extra spaces, characters, quotes or text. Please only produce 1 character. Answers with more than one characters will not be accepted.'.format(demographic_in_prompt)\n",
    "    elif output_type=='express_distribution': \n",
    "        prompt += 'After the examples, please express the distribution of answers from a group of \"{}\" for the question asked. Please only respond in the exact format of a dictionary mapping answer choice letter to probability, no extra spaces, characters, quotes or text. Please only produce 1 sentence in this format. Answers outside of this format will not be accepted.'.format(demographic_in_prompt)\n",
    "    question = data[qID]['question_text']\n",
    "    example_input = prompt + \"\\nQuestion: \" + question + \"?\\n\"\n",
    "    n = (sum(data[qID][demographic].values()))\n",
    "    MC_options = list(data[qID][demographic].keys())\n",
    "    for i, option in enumerate(MC_options):\n",
    "        example_input +=\"{}. {}. \".format(options[i], option)\n",
    "    example_input+=\"\\nAnswer:\"\n",
    "    return example_input\n",
    "    \n",
    "def get_test_questions_with_distributions(\n",
    "    seen_qIDs,\n",
    "    wave=\"Pew_American_Trends_Panel_disagreement_500\", \n",
    "    demographic_group=\"POLPARTY\",\n",
    "    demographic=\"Democrat\",\n",
    "):\n",
    "    data_path = '{}/opinions_qa/data/human_resp/'.format(os.getcwd())\n",
    "    demographic_in_prompt = demographic\n",
    "    data = json.load(open(data_path + wave + '/' + demographic_group + \"_data.json\"))\n",
    "    filtered_data = {}\n",
    "    for k, v in data.items():\n",
    "        if k in wave:\n",
    "            continue\n",
    "        filtered_data[k] = v\n",
    "    return filtered_data\n",
    "\n",
    "def parse_answers(raw_response, available_choices):\n",
    "    if \"Answer:\" not in raw_response:\n",
    "        print(\"Warning: Input string does not contain 'Answer:'.\")\n",
    "        return None\n",
    "    answers_part = raw_response.split(\"Answer:\")[1]\n",
    "    answers_list = answers_part.strip().split()\n",
    "    counts = {choice: 0 for choice in available_choices}\n",
    "    total_answers = 0\n",
    "    for answer in answers_list:\n",
    "        if answer in available_choices:\n",
    "            counts[answer] += 1\n",
    "            total_answers += 1\n",
    "        else:\n",
    "            # Optionally, handle invalid choices here\n",
    "            pass\n",
    "    probabilities = {choice: count / total_answers for choice, count in counts.items()}\n",
    "    return counts, probabilities\n",
    "\n",
    "def get_few_shot_contrastive_inputs(\n",
    "    q_ID,\n",
    "    wave=\"Pew_American_Trends_Panel_disagreement_100\", \n",
    "    demographic_group=\"POLPARTY\",\n",
    "    demographic=\"Democrat\",\n",
    "    output_type=\"model_logprobs\",\n",
    "    n_shots=5,\n",
    "    n_simulations_per_shot=1,\n",
    "    provide_ground_truth_distribution=False,\n",
    "):\n",
    "    data_path = '{}/opinions_qa/data/human_resp/'.format(os.getcwd())\n",
    "    demographic_in_prompt = demographic\n",
    "    data = json.load(open(data_path + wave + '/' + demographic_group + \"_data.json\"))\n",
    "    prompt = \"Your task is to simulate an answer to a new question. \"\n",
    "\n",
    "    if output_type=='sequence':\n",
    "        prompt+= 'After the examples, please simulate 30 samples for the new question asked. Please only respond with 30 multiple choice answers, no extra spaces, characters, quotes or text. Please only produce 30 characters. Answers with more than 30 characters will not be accepted.'\n",
    "    elif output_type=='model_logprobs': \n",
    "        prompt += 'After the examples, please simulate an answer for the question asked. Please only respond with a single multiple choice answer, no extra spaces, characters, quotes or text. Please only produce 1 character. Answers with more than one characters will not be accepted.'\n",
    "    elif output_type=='express_distribution': \n",
    "        prompt += 'After the examples, please express the distribution of answers for the question asked. Please only respond in the exact format of a dictionary mapping answer choice letter to probability, no extra spaces, characters, quotes or text. Please only produce 1 sentence in this format. Answers outside of this format will not be accepted.'\n",
    "\n",
    "    # we need the larger set to get icl demos\n",
    "    if wave == 'Pew_American_Trends_Panel_disagreement_100':\n",
    "        icl_wave='Pew_American_Trends_Panel_disagreement_500'\n",
    "    icl_data = json.load(open(data_path + icl_wave + '/' + demographic_group + \"_data.json\"))\n",
    "\n",
    "    # get icl qids\n",
    "    ICL_qIDS = get_ICL_qIDs(\n",
    "        q_ID=q_ID, wave=icl_wave, \n",
    "        demographic_group=demographic_group, demographic=demographic)\n",
    "\n",
    "    examples = []\n",
    "    \n",
    "    for icl_qID in ICL_qIDS[:n_shots]:\n",
    "        if icl_qID == q_ID:\n",
    "            continue\n",
    "\n",
    "        n = (sum(icl_data[icl_qID][demographic].values()))\n",
    "        MC_options = list(icl_data[icl_qID][demographic].keys())\n",
    "        all_options, probs = [], []\n",
    "        for i, option in enumerate(MC_options):\n",
    "            all_options.append(options[i])\n",
    "            probs.append(icl_data[icl_qID][demographic][option]/n)\n",
    "            if provide_ground_truth_distribution:\n",
    "                prompt +=\"{} be {}%, \".format(option, int((icl_data[icl_qID][demographic][option]/n)*100))\n",
    "\n",
    "        example_input = prompt + \"\\nQuestion: \" + icl_data[icl_qID]['question_text'] + \"?\\n\"\n",
    "        for i, option in enumerate(MC_options):\n",
    "            example_input +=\"{}. {}. \".format(options[i], option)\n",
    "        example_input+=\"\\nAnswer:\"\n",
    "        for _ in range(n_simulations_per_shot):\n",
    "            examples.append([example_input, q_ID, icl_qID, demographic_group, demographic, output_type, wave])\n",
    "\n",
    "    return pd.DataFrame(examples, columns=[\n",
    "        'input', 'qID', 'icl_qID', 'demographic_group', 'demographic', 'output_type', 'wave'\n",
    "    ])\n",
    "\n",
    "@torch.no_grad()\n",
    "def set_decoder_norm_to_unit_norm(model):\n",
    "    assert model.proj.weight is not None, \"Decoder weight was not initialized.\"\n",
    "\n",
    "    eps = torch.finfo(model.proj.weight.dtype).eps\n",
    "    norm = torch.norm(model.proj.weight.data, dim=1, keepdim=True)\n",
    "    model.proj.weight.data /= norm + eps\n",
    "\n",
    "def gather_residual_activations(model, target_layer, inputs):\n",
    "  target_act = None\n",
    "  def gather_target_act_hook(mod, inputs, outputs):\n",
    "    nonlocal target_act # make sure we can modify the target_act from the outer scope\n",
    "    target_act = outputs[0]\n",
    "    return outputs\n",
    "  handle = model.model.layers[target_layer].register_forward_hook(\n",
    "      gather_target_act_hook, always_call=True)\n",
    "  _ = model.forward(**inputs)\n",
    "  handle.remove()\n",
    "  return target_act\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, low_rank_dimension):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # Linear layer: input_dim -> 1 output (since binary classification)\n",
    "        self.proj = torch.nn.Linear(input_dim, low_rank_dimension)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "@dataclass\n",
    "class DataCollator(object):\n",
    "    \"\"\"Collate examples for ReFT.\"\"\"\n",
    "    \n",
    "    tokenizer: transformers.AutoTokenizer\n",
    "    data_collator: transformers.DataCollator\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        max_intervention_len = max([len(inst[\"intervention_locations\"][0]) for inst in instances])\n",
    "        max_seq_len = max([len(inst[\"input_ids\"]) for inst in instances])\n",
    "        \n",
    "        for inst in instances:\n",
    "            non_pad_len = len(inst[\"input_ids\"])\n",
    "\n",
    "            _intervention_mask = torch.ones_like(inst[\"intervention_locations\"][0])\n",
    "            _intervention_location_paddings = torch.tensor(\n",
    "                [[len(inst[\"input_ids\"]) for _ in range(max_intervention_len - len(inst[\"intervention_locations\"][0]))]])\n",
    "            _intervention_mask_paddings = torch.tensor(\n",
    "                [0 for _ in range(max_intervention_len - len(inst[\"intervention_locations\"][0]))])\n",
    "            inst[\"intervention_locations\"] = torch.cat([inst[\"intervention_locations\"], _intervention_location_paddings], dim=-1).int()\n",
    "            inst[\"intervention_masks\"] = torch.cat([_intervention_mask, _intervention_mask_paddings], dim=-1).int()\n",
    "\n",
    "            _input_id_paddings = torch.tensor(\n",
    "                [self.tokenizer.pad_token_id for _ in range(max_seq_len - non_pad_len)])\n",
    "            inst[\"input_ids\"] = torch.cat((inst[\"input_ids\"], torch.tensor([self.tokenizer.pad_token_id]), _input_id_paddings)).int()\n",
    "            inst[\"attention_mask\"] = (inst[\"input_ids\"] != self.tokenizer.pad_token_id).int()\n",
    "            inst[\"labels\"] = inst[\"labels\"].int()\n",
    "        batch_inputs = self.data_collator(instances)\n",
    "        return batch_inputs\n",
    "\n",
    "def make_data_module(\n",
    "    tokenizer: transformers.PreTrainedTokenizer, model, df, prefix_length=1\n",
    "):\n",
    "    all_input_ids, all_labels, all_intervention_locations = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        input_ids = tokenizer(\n",
    "            row[\"input\"], max_length=1024, truncation=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        base_length = len(input_ids)\n",
    "        intervention_locations = torch.tensor([[i for i in range(prefix_length, base_length)]])\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_labels.append(row[\"labels\"])\n",
    "        all_intervention_locations.append(intervention_locations)\n",
    "\n",
    "    train_dataset = datasets.Dataset.from_dict({\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"labels\": all_labels,\n",
    "        \"intervention_locations\": all_intervention_locations\n",
    "    })\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'labels', 'intervention_locations'])\n",
    "\n",
    "    data_collator_fn = transformers.DefaultDataCollator(\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    data_collator = DataCollator(tokenizer=tokenizer, data_collator=data_collator_fn)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n",
    "\n",
    "def make_dataloader(tokenizer, model, examples):\n",
    "    data_module = make_data_module(tokenizer, model, examples)\n",
    "    train_dataloader = DataLoader(\n",
    "        data_module[\"train_dataset\"], \n",
    "        shuffle=True, batch_size=8, \n",
    "        collate_fn=data_module[\"data_collator\"])\n",
    "    return train_dataloader\n",
    "\n",
    "@torch.no_grad()\n",
    "def train(tokenizer, model, ax, examples, layer, prefix_length=4):\n",
    "    train_dataloader = make_dataloader(tokenizer, model, examples)\n",
    "    torch.cuda.empty_cache()\n",
    "    ax.eval()\n",
    "    # Main training loop.\n",
    "    positive_activations = []\n",
    "    negative_activations = []\n",
    "    for batch in train_dataloader:\n",
    "        # prepare input\n",
    "        inputs = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        activations = gather_residual_activations(\n",
    "            model, layer, \n",
    "            {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]}\n",
    "        ).detach()\n",
    "        nonbos_mask = inputs[\"attention_mask\"][:,prefix_length:]\n",
    "        activations = activations[:,prefix_length:][nonbos_mask.bool()]\n",
    "        labels = inputs[\"labels\"].unsqueeze(1).repeat(\n",
    "            1, inputs[\"input_ids\"].shape[1] - prefix_length)\n",
    "        positive_activations.append(activations[labels[nonbos_mask.bool()] == 1])\n",
    "        negative_activations.append(activations[labels[nonbos_mask.bool()] != 1])\n",
    "\n",
    "    mean_positive_activation = torch.cat(positive_activations, dim=0).mean(dim=0)\n",
    "    mean_negative_activation = torch.cat(negative_activations, dim=0).mean(dim=0)\n",
    "    ax.proj.weight.data = mean_positive_activation.unsqueeze(0) - mean_negative_activation.unsqueeze(0)\n",
    "    set_decoder_norm_to_unit_norm(ax)\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "class AdditionIntervention(\n",
    "    SourcelessIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        # Note that we initialise these to zeros because we're loading in pre-trained weights.\n",
    "        # If you want to train your own SAEs then we recommend using blah\n",
    "        super().__init__(**kwargs, keep_last_dim=True)\n",
    "        self.proj = torch.nn.Linear(\n",
    "                self.embed_dim, kwargs[\"low_rank_dimension\"], bias=True)\n",
    "\n",
    "    def forward(self, base, source=None, subspaces=None):\n",
    "        # use subspaces[\"idx\"] to select the correct weight vector\n",
    "        steering_vec = subspaces[\"max_act\"] * subspaces[\"mag\"] * self.proj.weight[subspaces[\"idx\"]].unsqueeze(dim=0)\n",
    "        output = base + steering_vec.unsqueeze(dim=1)\n",
    "        return output\n",
    "\n",
    "def get_logits(tokenizer, model, ax, concept_id=0, k=10):\n",
    "    top_logits, neg_logits = [None], [None]\n",
    "    if concept_id is not None:\n",
    "        W_U = model.lm_head.weight.T\n",
    "        W_U = W_U * (model.model.norm.weight +\n",
    "                    torch.ones_like(model.model.norm.weight))[:, None]\n",
    "        W_U -= einops.reduce(\n",
    "            W_U, \"d_model d_vocab -> 1 d_vocab\", \"mean\"\n",
    "        )\n",
    "\n",
    "        vocab_logits = ax.proj.weight.data[concept_id] @ W_U\n",
    "        top_values, top_indices = vocab_logits.topk(k=k, sorted=True)\n",
    "        top_tokens = tokenizer.batch_decode(top_indices.unsqueeze(dim=-1))\n",
    "        top_logits = [list(zip(top_tokens, top_values.tolist()))]\n",
    "        \n",
    "        neg_values, neg_indices = vocab_logits.topk(k=k, largest=False, sorted=True)\n",
    "        neg_tokens = tokenizer.batch_decode(neg_indices.unsqueeze(dim=-1))\n",
    "        neg_logits = [list(zip(neg_tokens, neg_values.tolist()))]\n",
    "    return top_logits, neg_logits\n",
    "\n",
    "def parse_answers(raw_response, available_choices):\n",
    "    \"\"\"\n",
    "    Parse the answers from a raw response string and calculate counts and probabilities.\n",
    "\n",
    "    Args:\n",
    "        raw_response (str): The raw input string containing the answers.\n",
    "        available_choices (list): A list of valid answer choices (e.g., [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (status, result)\n",
    "            - status (bool): True if successful, False if an error occurs.\n",
    "            - result: A dictionary containing counts and probabilities if successful,\n",
    "                      or an error message if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"Answer:\" not in raw_response:\n",
    "            raise ValueError(\"No 'Answer:' keyword found in input.\")\n",
    "        answers_part = raw_response.split(\"Answer:\")[1]\n",
    "        answers_list = answers_part.strip().split()\n",
    "        if not answers_list:\n",
    "            raise ValueError(\"No parsable answers found in input.\")\n",
    "        counts = {choice: 0 for choice in available_choices}\n",
    "        total_answers = 0\n",
    "\n",
    "        for answer in answers_list:\n",
    "            if answer in available_choices:\n",
    "                counts[answer] += 1\n",
    "                total_answers += 1\n",
    "            else:\n",
    "                # Skip invalid choices\n",
    "                pass\n",
    "        if total_answers < 3:\n",
    "            raise ZeroDivisionError(\"Not enough valid answers to calculate probabilities.\")\n",
    "        probabilities = {choice: count / total_answers for choice, count in counts.items()}\n",
    "        return True, {\"counts\": counts, \"probabilities\": probabilities}\n",
    "\n",
    "    except ValueError as ve:\n",
    "        return False, {\"message\": str(ve)}\n",
    "    except ZeroDivisionError as zde:\n",
    "        return False, {\"message\": str(zde)}\n",
    "    except Exception as e:\n",
    "        return False, {\"message\": f\"Unexpected error: {str(e)}\"}\n",
    "\n",
    "def calculate_kld(golden_distribution, predicted_distribution):\n",
    "    golden_probs = np.array([golden_distribution[key] for key in golden_distribution])\n",
    "    predicted_probs = np.array([predicted_distribution[key] for key in golden_distribution])\n",
    "    epsilon = 1e-12\n",
    "    golden_probs = np.clip(golden_probs, epsilon, 1)\n",
    "    predicted_probs = np.clip(predicted_probs, epsilon, 1)\n",
    "    kld = np.sum(golden_probs * np.log(golden_probs / predicted_probs))\n",
    "    return kld\n",
    "\n",
    "def calculate_jsd(golden_distribution, predicted_distribution):\n",
    "    golden_probs = np.array([golden_distribution[key] for key in golden_distribution])\n",
    "    predicted_probs = np.array([predicted_distribution[key] for key in golden_distribution])\n",
    "    epsilon = 1e-12\n",
    "    golden_probs = np.clip(golden_probs, epsilon, 1)\n",
    "    predicted_probs = np.clip(predicted_probs, epsilon, 1)\n",
    "    m = 0.5 * (golden_probs + predicted_probs)\n",
    "    kl_golden_to_m = np.sum(golden_probs * np.log(golden_probs / m))\n",
    "    kl_predicted_to_m = np.sum(predicted_probs * np.log(predicted_probs / m))\n",
    "    jsd = 0.5 * (kl_golden_to_m + kl_predicted_to_m)\n",
    "    return jsd\n",
    "\n",
    "def compute_kld_values(golden_distribution, sampled_distributions):\n",
    "    return [calculate_kld(golden_distribution, dist) for dist in sampled_distributions]\n",
    "\n",
    "def compute_jsd_values(golden_distribution, sampled_distributions):\n",
    "    return [calculate_jsd(golden_distribution, dist) for dist in sampled_distributions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cf2a67c-9281-4f2b-a58a-e45ab01834e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9049174a0014cc79290fb2a78a01cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a549e6bcab9c457386a366f8ecd2934b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model_name_or_path = \"google/gemma-2-2b-it\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=2048, \n",
    "    padding_side=\"right\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "97ca9586-fdb7-47bf-9d22-c7b1193a171e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>qID</th>\n",
       "      <th>icl_qID</th>\n",
       "      <th>demographic_group</th>\n",
       "      <th>demographic</th>\n",
       "      <th>output_type</th>\n",
       "      <th>wave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>B D B A A D B A D D D A D A B C A D D B A B C ...</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>INEQ5_f_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>D D D D E A D A D C D C C D D C C D C C D E C ...</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>ECON5_h_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>F D C E C E D C A F C F D D D E C D D C C D D ...</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>ECON5_i_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "1  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "2  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "\n",
       "                                              output          qID  \\\n",
       "0  B D B A A D B A D D D A D A B C A D D B A B C ...  ECON5_d_W54   \n",
       "1  D D D D E A D A D C D C C D D C C D C C D E C ...  ECON5_d_W54   \n",
       "2  F D C E C E D C A F C F D D D E C D D C C D D ...  ECON5_d_W54   \n",
       "\n",
       "       icl_qID demographic_group demographic output_type  \\\n",
       "0  INEQ5_f_W54          POLPARTY    Democrat    sequence   \n",
       "1  ECON5_h_W54          POLPARTY    Democrat    sequence   \n",
       "2  ECON5_i_W54          POLPARTY    Democrat    sequence   \n",
       "\n",
       "                                         wave  \n",
       "0  Pew_American_Trends_Panel_disagreement_100  \n",
       "1  Pew_American_Trends_Panel_disagreement_100  \n",
       "2  Pew_American_Trends_Panel_disagreement_100  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_group = \"POLPARTY\"\n",
    "demographic = \"Democrat\"\n",
    "output_type = \"sequence\"\n",
    "\n",
    "qIDs, waves = get_q_IDs()\n",
    "raw_dataset = get_few_shot_training_examples(\n",
    "    qIDs[0],\n",
    "    wave=\"Pew_American_Trends_Panel_disagreement_100\", \n",
    "    demographic_group=demographic_group,\n",
    "    demographic=demographic,\n",
    "    output_type=output_type, \n",
    "    n_shots=5,\n",
    "    n_simulations_per_shot=1,\n",
    ")\n",
    "training_dataset = prepare_df(raw_dataset.copy(), tokenizer)\n",
    "training_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9f2fe470-4142-4cfa-ab3a-89e2fbeb880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "contrastive_df = get_few_shot_contrastive_inputs(\n",
    "    qIDs[0],\n",
    "    wave=\"Pew_American_Trends_Panel_disagreement_100\", \n",
    "    demographic_group=demographic_group,\n",
    "    demographic=demographic,\n",
    "    output_type=output_type, \n",
    "    n_shots=5,\n",
    "    n_simulations_per_shot=1,\n",
    ")\n",
    "all_responses = []\n",
    "for index, row in contrastive_df.iterrows():\n",
    "    instruction = apply_chat_template({\"input\": row[\"input\"]})\n",
    "    prompt = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
    "    model_response = model.generate(\n",
    "        **prompt, max_new_tokens=64, do_sample=True, \n",
    "        eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    "    )\n",
    "    response = tokenizer.decode(model_response[0][prompt[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    all_responses += [response.strip()]\n",
    "contrastive_df[\"output\"] = all_responses\n",
    "contrastive_dataset = prepare_df(contrastive_df.copy(), tokenizer)\n",
    "training_dataset[\"labels\"] = 1\n",
    "contrastive_dataset[\"labels\"] = 0\n",
    "contrastive_training_dataset = pd.concat([training_dataset, contrastive_dataset]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6cdc2fa5-a41c-4d7c-abe5-184e7931b263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>qID</th>\n",
       "      <th>icl_qID</th>\n",
       "      <th>demographic_group</th>\n",
       "      <th>demographic</th>\n",
       "      <th>output_type</th>\n",
       "      <th>wave</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>B D B A A D B A D D D A D A B C A D D B A B C ...</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>INEQ5_f_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>D D D D E A D A D C D C C D D C C D C C D E C ...</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>ECON5_h_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>F D C E C E D C A F C F D D D E C D D C C D D ...</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>ECON5_i_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>D D D D D D D D D F D C B D C D A D A D C C D ...</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>ECON5_j_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>D C B A D A A C C C A B B C C D A A B A E C B ...</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>ECON5_k_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>E.F\\nA.B.C.D.E. \\nA.B.C.D.E. \\nA.B.C.D.E. \\nA....</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>INEQ5_f_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>A \\nB\\nC\\nD\\nE\\nF \\nA\\nB\\nC\\nD\\nE\\nF\\nA\\nB\\nC\\...</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>ECON5_h_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>A B C D E</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>ECON5_i_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>E \\nA \\nB \\nC \\nD \\nE \\nF \\nA \\nB \\nC \\nD \\nE ...</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>ECON5_j_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYour task is to simulate ...</td>\n",
       "      <td>A. \\nB. \\nC. \\nD. \\nE. \\nF. \\nA. \\nB. \\nC. \\nD...</td>\n",
       "      <td>ECON5_d_W54</td>\n",
       "      <td>ECON5_k_W54</td>\n",
       "      <td>POLPARTY</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>sequence</td>\n",
       "      <td>Pew_American_Trends_Panel_disagreement_100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "1  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "2  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "3  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "4  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "5  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "6  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "7  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "8  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "9  <start_of_turn>user\\nYour task is to simulate ...   \n",
       "\n",
       "                                              output          qID  \\\n",
       "0  B D B A A D B A D D D A D A B C A D D B A B C ...  ECON5_d_W54   \n",
       "1  D D D D E A D A D C D C C D D C C D C C D E C ...  ECON5_d_W54   \n",
       "2  F D C E C E D C A F C F D D D E C D D C C D D ...  ECON5_d_W54   \n",
       "3  D D D D D D D D D F D C B D C D A D A D C C D ...  ECON5_d_W54   \n",
       "4  D C B A D A A C C C A B B C C D A A B A E C B ...  ECON5_d_W54   \n",
       "5  E.F\\nA.B.C.D.E. \\nA.B.C.D.E. \\nA.B.C.D.E. \\nA....  ECON5_d_W54   \n",
       "6  A \\nB\\nC\\nD\\nE\\nF \\nA\\nB\\nC\\nD\\nE\\nF\\nA\\nB\\nC\\...  ECON5_d_W54   \n",
       "7                                          A B C D E  ECON5_d_W54   \n",
       "8  E \\nA \\nB \\nC \\nD \\nE \\nF \\nA \\nB \\nC \\nD \\nE ...  ECON5_d_W54   \n",
       "9  A. \\nB. \\nC. \\nD. \\nE. \\nF. \\nA. \\nB. \\nC. \\nD...  ECON5_d_W54   \n",
       "\n",
       "       icl_qID demographic_group demographic output_type  \\\n",
       "0  INEQ5_f_W54          POLPARTY    Democrat    sequence   \n",
       "1  ECON5_h_W54          POLPARTY    Democrat    sequence   \n",
       "2  ECON5_i_W54          POLPARTY    Democrat    sequence   \n",
       "3  ECON5_j_W54          POLPARTY    Democrat    sequence   \n",
       "4  ECON5_k_W54          POLPARTY    Democrat    sequence   \n",
       "5  INEQ5_f_W54          POLPARTY    Democrat    sequence   \n",
       "6  ECON5_h_W54          POLPARTY    Democrat    sequence   \n",
       "7  ECON5_i_W54          POLPARTY    Democrat    sequence   \n",
       "8  ECON5_j_W54          POLPARTY    Democrat    sequence   \n",
       "9  ECON5_k_W54          POLPARTY    Democrat    sequence   \n",
       "\n",
       "                                         wave  labels  \n",
       "0  Pew_American_Trends_Panel_disagreement_100       1  \n",
       "1  Pew_American_Trends_Panel_disagreement_100       1  \n",
       "2  Pew_American_Trends_Panel_disagreement_100       1  \n",
       "3  Pew_American_Trends_Panel_disagreement_100       1  \n",
       "4  Pew_American_Trends_Panel_disagreement_100       1  \n",
       "5  Pew_American_Trends_Panel_disagreement_100       0  \n",
       "6  Pew_American_Trends_Panel_disagreement_100       0  \n",
       "7  Pew_American_Trends_Panel_disagreement_100       0  \n",
       "8  Pew_American_Trends_Panel_disagreement_100       0  \n",
       "9  Pew_American_Trends_Panel_disagreement_100       0  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bc39f76b-100a-4446-9bd0-867a7a49597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "LAYER = 20\n",
    "\n",
    "ax = LogisticRegressionModel(model.config.hidden_size, 1)\n",
    "ax = ax.to(DEVICE)\n",
    "train(tokenizer, model, ax, contrastive_training_dataset, LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ddf05f05-58ef-4834-966e-09a11d9ed993",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_ax = AdditionIntervention(\n",
    "    embed_dim=model.config.hidden_size, \n",
    "    low_rank_dimension=1,\n",
    ")\n",
    "steering_ax.proj.weight.data = ax.proj.weight.data\n",
    "steering_config = IntervenableConfig(representations=[{\n",
    "    \"layer\": LAYER,\n",
    "    \"component\": f\"model.layers[{LAYER}].output\",\n",
    "    \"low_rank_dimension\": 1,\n",
    "    \"intervention\": steering_ax} for l in [LAYER]])\n",
    "steering_model = IntervenableModel(steering_config, model)\n",
    "steering_model.set_device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1c4d59e3-221f-477c-86bc-89c4356e441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing qID: RACESURV17_W43\n",
      "Golden dist:\n",
      "{'A': 0.3181818181818182, 'B': 0.5307802433786686, 'C': 0.048675733715103794, 'D': 0.09663564781675017, 'E': 0.00572655690765927}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wuzhengx/ipykernel_1678893/1983615292.py:4: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  test_qID = random.sample(test_pool.keys(), 1)[0]\n"
     ]
    }
   ],
   "source": [
    "test_pool = get_test_questions_with_distributions(\n",
    "    seen_qIDs=set(training_dataset.qID).union(training_dataset.icl_qID)\n",
    ")\n",
    "test_qID = random.sample(test_pool.keys(), 1)[0]\n",
    "# test_qID = \"ECON5_d_W54\"\n",
    "print(\"testing qID:\", test_qID)\n",
    "n = (sum(test_pool[test_qID][demographic].values()))\n",
    "MC_options = list(test_pool[test_qID][demographic].keys())\n",
    "all_options, probs = [], []\n",
    "for i, option in enumerate(MC_options):\n",
    "    all_options.append(options[i])\n",
    "    probs.append(test_pool[test_qID][demographic][option]/n)\n",
    "golden_dist = dict(zip(all_options, probs))\n",
    "print(\"Golden dist:\")\n",
    "print(golden_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9636e846-a543-4338-ad67-a162a7028743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instruction = apply_zeroshot_prompt_template(test_qID, output_type=\"sequence\")\n",
    "instruction = apply_chat_template({\"input\": instruction})\n",
    "prompt = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1  # last position\n",
    "_, reft_response = steering_model.generate(\n",
    "    prompt, unit_locations=None, intervene_on_prompt=True, \n",
    "    subspaces=[{\"mag\": 2, \"max_act\": 100, \"idx\": 0}], max_new_tokens=32, do_sample=True, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "response = tokenizer.decode(reft_response[0][prompt[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a3e13a00-a5f4-43b1-a5a1-cb613ba8cf22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[(' Democrats', 0.859375),\n",
       "   (' Democratic', 0.84375),\n",
       "   ('NUMX', 0.8125),\n",
       "   (' Biden', 0.76171875),\n",
       "   (' Democrat', 0.7578125),\n",
       "   ('++\\r', 0.7578125),\n",
       "   (' caucus', 0.73828125),\n",
       "   (' Congressional', 0.7265625),\n",
       "   (' Republicans', 0.72265625),\n",
       "   (' congressional', 0.71484375)]],\n",
       " [[('AddTagHelper', -0.5625),\n",
       "   (' manusia', -0.5546875),\n",
       "   ('المكان', -0.53125),\n",
       "   (' Organisms', -0.515625),\n",
       "   (' aiut', -0.50390625),\n",
       "   ('formazioni', -0.50390625),\n",
       "   ('mberto', -0.50390625),\n",
       "   (' svolge', -0.498046875),\n",
       "   (' AssemblyCulture', -0.49609375),\n",
       "   (' visuales', -0.49609375)]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_logits(tokenizer, model, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4a819-9d75-49f7-83cb-fab0e2931ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
