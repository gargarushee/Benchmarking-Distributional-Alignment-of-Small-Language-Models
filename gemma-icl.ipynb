{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c93d21fc-78d7-4f13-a588-b70a379bf422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0706a81-7fb5-4ebc-b952-5885b0f7d294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78864c90ee9848168562427c54e2a930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653ad3f9b5db4237957f9b1d38686b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers\n",
    "device = \"cuda\"\n",
    "\n",
    "model_name_or_path = \"google/gemma-2-2b-it\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=2048, \n",
    "    padding_side=\"right\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c29d22-70d3-44f1-a231-559c0d90dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run random baseline to get these files first!\n",
    "n_training_qIDs = \"train_qIDs.json\"\n",
    "n_testing_qIDs = \"test_qIDs.json\"\n",
    "\n",
    "# demographic group and output type\n",
    "demographic_group = \"POLPARTY\"\n",
    "demographic = \"Republican\"\n",
    "output_type = \"sequence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3672c86-7a1d-497d-bae4-a78f4e9f8776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: WHYNOTBIZF2G_W36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: GAP21Q33_r_W82\n",
      "Evaluating: NEIGHINTERA_W32\n",
      "Evaluating: FUTRCLASSc_W41\n",
      "Evaluating: TRAITPOLMF1B_W36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/juice2/scr2/wuzhengx/cs329h/Benchmarking-Distributional-Alignment-of-Small-Language-Models/utils.py:138: RuntimeWarning: invalid value encountered in divide\n",
      "  icl_values = np.array(icl_values)/np.sum(icl_values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: FUD37A_W34\n",
      "Evaluating: HIGHEDWRNGB_W36\n",
      "Evaluating: WHYNOTPOLF1C_W36\n",
      "Evaluating: GAP21Q4_f_W82\n",
      "Evaluating: ESSENPOLF1B_W36\n",
      "Evaluating: RQ4_F1Ba_W42\n",
      "Evaluating: RACESURV14_W43\n",
      "Evaluating: INFOCREATEa_W45\n",
      "Evaluating: GAP21Q19_a_W82\n",
      "Evaluating: GROWUPVIOL_W26\n",
      "Evaluating: FAMSURV23e_W50\n",
      "Evaluating: GUNTYPEOWNC_W26\n",
      "Evaluating: ROMRELDUR_W50\n",
      "Evaluating: GAP21Q31_W82\n",
      "Evaluating: BILLION_W92\n",
      "Success rate: 1.0\n"
     ]
    }
   ],
   "source": [
    "def apply_chat_template(row):\n",
    "    messages = [{\"role\": \"user\", \"content\": row[\"input\"]}]\n",
    "    nobos = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)[1:]\n",
    "    return tokenizer.decode(nobos)\n",
    "\n",
    "test_pool = get_test_questions_with_distributions(\n",
    "    seen_qIDs={}, \n",
    "    demographic_group=demographic_group,\n",
    "    demographic=demographic,\n",
    ")\n",
    "test_qIDs = json.load(open(n_testing_qIDs))\n",
    "\n",
    "k = 1\n",
    "success_rates = []\n",
    "probabilities_list = []\n",
    "for test_qID in test_qIDs:\n",
    "    print(\"Evaluating:\", test_qID)\n",
    "    # test_qID = \"ECON5_d_W54\"\n",
    "    n = (sum(test_pool[test_qID][demographic].values()))\n",
    "    MC_options = list(test_pool[test_qID][demographic].keys())\n",
    "    all_options, probs = [], []\n",
    "    for i, option in enumerate(MC_options):\n",
    "        all_options.append(options[i])\n",
    "        probs.append(test_pool[test_qID][demographic][option]/n)\n",
    "    golden_dist = dict(zip(all_options, probs))\n",
    "    # print(\"Golden dist:\")\n",
    "    # print(golden_dist)\n",
    "\n",
    "    instruction = get_icl_prompt_opinionqa(\n",
    "        test_qID,\n",
    "        demographic_group=demographic_group,\n",
    "        demographic=demographic,\n",
    "        output_type=output_type\n",
    "    )\n",
    "    \n",
    "    instruction = apply_chat_template({\"input\": instruction})\n",
    "    model_inputs = tokenizer(instruction, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    successful_parsings = 0\n",
    "    total_attempts = 0\n",
    "    while successful_parsings < k:\n",
    "        outputs = model.generate(\n",
    "            **model_inputs, max_new_tokens=36, do_sample=True, \n",
    "            eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0][model_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "        # print(response)\n",
    "        success, result = parse_answers(response, all_options, answer_tag=False)\n",
    "        total_attempts += 1\n",
    "        if success:\n",
    "            successful_parsings += 1\n",
    "            probabilities_list.append([golden_dist, result[\"probabilities\"]])\n",
    "        success_rate = successful_parsings / total_attempts\n",
    "        success_rates += [success_rate]\n",
    "success_rate = np.array(success_rates).mean()\n",
    "print(\"Success rate:\", success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7daae4e7-69c1-4921-8e80-cf8d63d9154a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8153693915522485"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = compute_l1_values(probabilities_list)\n",
    "json.dump(distances, open(\"distance_icl.json\", \"w\"))\n",
    "np.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5e547-57c1-45b4-9a12-02c1dca77b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
